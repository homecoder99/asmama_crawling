#!/usr/bin/env python3
"""
í¬ë¡¤ëŸ¬ ê¸°ë³¸ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ í”Œë ˆì´ê·¸ë¼ìš´ë“œ.

ì‚¬ìš©ë²•:
    python playground/test_crawler.py --branduid=1234567
    python playground/test_crawler.py --help
"""

import sys
import argparse
import asyncio
from pathlib import Path
from urllib.parse import urlparse

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.insert(0, str(Path(__file__).parent.parent))

from crawler.asmama import AsmamaCrawler
from crawler.storage import JSONStorage
from crawler.utils import setup_logger

logger = setup_logger(__name__)

async def test_crawl_from_list(list_url: str, output_dir: str = "playground/results"):
    """
    ë¦¬ìŠ¤íŠ¸ í˜ì´ì§€ í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸.
    """
    print(f"ğŸ” ë¦¬ìŠ¤íŠ¸ í˜ì´ì§€ í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸ ì‹œì‘: {list_url}")
    
    # ê²°ê³¼ ë””ë ‰í† ë¦¬ ìƒì„±
    results_dir = Path(output_dir)
    results_dir.mkdir(parents=True, exist_ok=True)

    # í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”
    crawler = AsmamaCrawler(max_workers=1)
    
    try:
        async with crawler:
            print("ğŸ“¡ ë¸Œë¼ìš°ì € ì´ˆê¸°í™” ì™„ë£Œ")
            
            # branduid ëª©ë¡ í¬ë¡¤ë§
            branduid_list = await crawler.crawl_branduid_list(list_url)

            # branduid ëª©ë¡ì—ì„œ ì œí’ˆ í¬ë¡¤ë§
            product_data = await crawler.crawl_from_branduid_list(branduid_list)
            
            print(f"ğŸ” ì´ {len(product_data)}ê°œ ì œí’ˆ í¬ë¡¤ë§ ì™„ë£Œ")

            # ì €ì¥ì†Œ ì„¤ì •
            url_name = "_".join(urlparse(list_url).path.split("/")) + "_" + urlparse(list_url).query
            storage_path = results_dir / f"test_list{url_name}.json"
            storage = JSONStorage(str(storage_path))
            storage.save(product_data)

    except Exception as e:
        print(f"ğŸ’¥ í¬ë¡¤ëŸ¬ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        logger.error(f"í¬ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}", exc_info=True)

async def test_crawl_branduid_list(list_url: str, output_dir: str = "playground/results"):
    """
    ë¦¬ìŠ¤íŠ¸ í˜ì´ì§€ í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸.
    """
    print(f"ğŸ” ë¦¬ìŠ¤íŠ¸ í˜ì´ì§€ í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸ ì‹œì‘: {list_url}")
    
    # ê²°ê³¼ ë””ë ‰í† ë¦¬ ìƒì„±
    results_dir = Path(output_dir)
    results_dir.mkdir(parents=True, exist_ok=True)
    
    url_name = "_".join(urlparse(list_url).path.split("/")) + "_" + urlparse(list_url).query

    # ì €ì¥ì†Œ ì„¤ì •
    storage_path = results_dir / f"test_list{url_name}.json"
    storage = JSONStorage(str(storage_path))

    # í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”
    crawler = AsmamaCrawler(storage=storage, max_workers=1)
    
    try:
        async with crawler:
            print("ğŸ“¡ ë¸Œë¼ìš°ì € ì´ˆê¸°í™” ì™„ë£Œ")
            
            # ë¦¬ìŠ¤íŠ¸ í˜ì´ì§€ í¬ë¡¤ë§
            branduid_list = await crawler.crawl_branduid_list(list_url)
            
            print(f"ğŸ” ì´ {len(branduid_list)}ê°œ branduid ì¶”ì¶œ ì™„ë£Œ")
            
            # ì €ì¥
            storage.save(branduid_list)
 
    except Exception as e:
        print(f"ğŸ’¥ í¬ë¡¤ëŸ¬ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        logger.error(f"í¬ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}", exc_info=True)

async def test_single_product(branduid: str, output_dir: str = "playground/results"):
    """
    ë‹¨ì¼ ì œí’ˆ í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸.
    
    Args:
        branduid: í…ŒìŠ¤íŠ¸í•  ì œí’ˆì˜ branduid
        output_dir: ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬
    """
    print(f"ğŸ” ì œí’ˆ í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸ ì‹œì‘: branduid={branduid}")
    
    # ê²°ê³¼ ë””ë ‰í† ë¦¬ ìƒì„±
    results_dir = Path(output_dir)
    results_dir.mkdir(parents=True, exist_ok=True)
    
    # ì €ì¥ì†Œ ì„¤ì •
    storage_path = results_dir / f"test_product_{branduid}.json"
    storage = JSONStorage(str(storage_path))
    
    # í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”
    crawler = AsmamaCrawler(storage=storage, max_workers=1)
    
    try:
        async with crawler:
            print("ğŸ“¡ ë¸Œë¼ìš°ì € ì´ˆê¸°í™” ì™„ë£Œ")
            
            # ì œí’ˆ í¬ë¡¤ë§
            result = await crawler.crawl_single_product(branduid)
            
            if result:
                print("âœ… í¬ë¡¤ë§ ì„±ê³µ!")
                print(f"ğŸ“„ ì œí’ˆëª…: {result.get('item_name', 'N/A')}")
                print(f"ğŸ’° ê°€ê²©: {result.get('price', 'N/A')}")
                print(f"ğŸ¨ ì˜µì…˜ ìˆ˜: {len(result.get('option_info', []))}")
                print(f"ğŸ–¼ï¸  ì´ë¯¸ì§€ ìˆ˜: {len(result.get('images', []))}")
                print(f"ğŸ’¾ ì €ì¥ ìœ„ì¹˜: {storage_path}")

            else:
                print("âŒ í¬ë¡¤ë§ ì‹¤íŒ¨")
                print("ğŸ” ë¡œê·¸ íŒŒì¼ì„ í™•ì¸í•˜ì—¬ ìƒì„¸ ì˜¤ë¥˜ë¥¼ í™•ì¸í•˜ì„¸ìš”")
                
    except Exception as e:
        print(f"ğŸ’¥ í¬ë¡¤ëŸ¬ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        logger.error(f"í¬ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}", exc_info=True)


def test_crawler_initialization():
    """
    í¬ë¡¤ëŸ¬ ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸.
    """
    print("ğŸ”§ í¬ë¡¤ëŸ¬ ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸...")
    
    try:
        # ê¸°ë³¸ ì´ˆê¸°í™”
        crawler = AsmamaCrawler()
        print(f"âœ… ê¸°ë³¸ ì´ˆê¸°í™” ì„±ê³µ (max_workers: {crawler.max_workers})")
        
        # ì»¤ìŠ¤í…€ ì„¤ì •
        storage = JSONStorage("playground/results/test_init.json")
        crawler_custom = AsmamaCrawler(storage=storage, max_workers=1)
        print(f"âœ… ì»¤ìŠ¤í…€ ì´ˆê¸°í™” ì„±ê³µ (max_workers: {crawler_custom.max_workers})")
        
        return True
        
    except Exception as e:
        print(f"âŒ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
        return False


def analyze_results(results_dir: str = "playground/results"):
    """
    ì €ì¥ëœ ê²°ê³¼ ë¶„ì„.
    
    Args:
        results_dir: ê²°ê³¼ ë””ë ‰í† ë¦¬
    """
    print("ğŸ“Š ê²°ê³¼ ë¶„ì„...")
    
    results_path = Path(results_dir)
    if not results_path.exists():
        print("âŒ ê²°ê³¼ ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    json_files = list(results_path.glob("*.json"))
    
    if not json_files:
        print("âŒ ê²°ê³¼ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print(f"ğŸ“ ë°œê²¬ëœ ê²°ê³¼ íŒŒì¼: {len(json_files)}ê°œ")
    
    total_products = 0
    for json_file in json_files:
        try:
            storage = JSONStorage(str(json_file))
            data = storage.load()
            total_products += len(data)
            print(f"   ğŸ“„ {json_file.name}: {len(data)}ê°œ ì œí’ˆ")
        except Exception as e:
            print(f"   âŒ {json_file.name}: ì½ê¸° ì˜¤ë¥˜ - {str(e)}")
    
    print(f"ğŸ“ˆ ì´ í¬ë¡¤ë§ëœ ì œí’ˆ: {total_products}ê°œ")


def main():
    # """ë©”ì¸ í•¨ìˆ˜."""
    # parser = argparse.ArgumentParser(description="í¬ë¡¤ëŸ¬ ê¸°ë³¸ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸")
    # parser.add_argument(
    #     "--branduid",
    #     type=str,
    #     help="í…ŒìŠ¤íŠ¸í•  ì œí’ˆì˜ branduid"
    # )
    # parser.add_argument(
    #     "--init-test",
    #     action="store_true",
    #     help="í¬ë¡¤ëŸ¬ ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸ë§Œ ì‹¤í–‰"
    # )
    # parser.add_argument(
    #     "--analyze",
    #     action="store_true",
    #     help="ì €ì¥ëœ ê²°ê³¼ ë¶„ì„"
    # )
    # parser.add_argument(
    #     "--output-dir",
    #     default="playground/results",
    #     help="ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬"
    # )
    
    # args = parser.parse_args()
    
    # print("ğŸš€ í¬ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸ í”Œë ˆì´ê·¸ë¼ìš´ë“œ")
    # print("=" * 40)
    
    # if args.init_test:
    #     success = test_crawler_initialization()
    #     sys.exit(0 if success else 1)
    
    # if args.analyze:
    #     analyze_results(args.output_dir)
    #     return
    
    # if not args.branduid:
    #     print("âŒ --branduid ì¸ìê°€ í•„ìš”í•©ë‹ˆë‹¤.")
    #     print("ğŸ’¡ ì˜ˆì‹œ: python playground/test_crawler.py --branduid=1234567")
    #     sys.exit(1)
    
    # # í¬ë¡¤ëŸ¬ ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸
    # if not test_crawler_initialization():
    #     sys.exit(1)
    
    # # ì œí’ˆ í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸
    # asyncio.run(test_single_product(args.branduid, args.output_dir))

    """ë©”ì¸ í•¨ìˆ˜."""
    parser = argparse.ArgumentParser(description="í¬ë¡¤ëŸ¬ ë¦¬ìŠ¤íŠ¸ ì¶”ì¶œ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸")
    parser.add_argument(
        "--list-url",
        type=str,
        help="í…ŒìŠ¤íŠ¸í•  ë¦¬ìŠ¤íŠ¸ í˜ì´ì§€ URL"
    )
    parser.add_argument(
        "--output-dir",
        default="playground/results",
        help="ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬"
    )
    args = parser.parse_args()

    print("ğŸš€ í¬ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸ í”Œë ˆì´ê·¸ë¼ìš´ë“œ")
    print("=" * 40)

    if not args.list_url:
        print("âŒ --list-url ì¸ìê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        print("ğŸ’¡ ì˜ˆì‹œ: python playground/test_crawler.py --list-url=http://www.asmama.com/shop/bestseller.html?xcode=REVIEW")
        sys.exit(1)
    
    # í¬ë¡¤ëŸ¬ ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸
    if not test_crawler_initialization():
        sys.exit(1)
    
    # ì œí’ˆ í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸
    asyncio.run(test_crawl_from_list(args.list_url, args.output_dir))


if __name__ == "__main__":
    main()