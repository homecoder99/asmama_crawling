#!/usr/bin/env python3
"""
í¬ë¡¤ë§ ê²°ê³¼ ë°ì´í„° ë¶„ì„ í”Œë ˆì´ê·¸ë¼ìš´ë“œ.

Excel íŒŒì¼ì´ë‚˜ JSON íŒŒì¼ì—ì„œ í¬ë¡¤ë§ ê²°ê³¼ë¥¼ ë¡œë“œí•˜ì—¬ í†µê³„ ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.

ì‚¬ìš©ë²•:
    python playground/analyze_data.py --input=data/asmama_products.xlsx
    python playground/analyze_data.py --input=playground/results/*.json --format=json
"""

import sys
import argparse
import json
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple
import glob

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.insert(0, str(Path(__file__).parent.parent))

try:
    import pandas as pd
    PANDAS_AVAILABLE = True
except ImportError:
    PANDAS_AVAILABLE = False

from crawler.utils import setup_logger, parse_price
from crawler.validator import ProductValidator

logger = setup_logger(__name__)


def safe_str_check(value) -> bool:
    """
    ê°’ì´ ìœ íš¨í•œ ë¬¸ìì—´ì¸ì§€ ì•ˆì „í•˜ê²Œ í™•ì¸í•œë‹¤.
    
    Args:
        value: í™•ì¸í•  ê°’
        
    Returns:
        ìœ íš¨í•œ ë¬¸ìì—´ ì—¬ë¶€
    """
    if value is None:
        return False
    if isinstance(value, float):
        import math
        if math.isnan(value):
            return False
        return str(value).strip() != ""
    if isinstance(value, str):
        return value.strip() != ""
    if isinstance(value, (int, list)):
        return True
    return False


class DataAnalyzer:
    """í¬ë¡¤ë§ ë°ì´í„° ë¶„ì„ê¸°."""
    
    def __init__(self):
        """ë¶„ì„ê¸° ì´ˆê¸°í™”."""
        self.data: List[Dict[str, Any]] = []
        self.validation_log_path: Optional[str] = None
        self.validator: Optional[ProductValidator] = None
    
    def load_excel(self, file_path: str) -> bool:
        """
        Excel íŒŒì¼ì—ì„œ ë°ì´í„°ë¥¼ ë¡œë“œí•œë‹¤.
        
        Args:
            file_path: Excel íŒŒì¼ ê²½ë¡œ
            
        Returns:
            ë¡œë“œ ì„±ê³µ ì—¬ë¶€
        """
        if not PANDAS_AVAILABLE:
            print("âŒ pandasê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install pandasë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
            return False
        
        try:
            df = pd.read_excel(file_path)
            
            self.data = df.to_dict('records')
            print(f"âœ… Excel ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(self.data)}ê°œ í•­ëª©")
            return True
            
        except Exception as e:
            print(f"âŒ Excel íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
            return False
    
    def load_json_files(self, pattern: str) -> bool:
        """
        JSON íŒŒì¼ë“¤ì—ì„œ ë°ì´í„°ë¥¼ ë¡œë“œí•œë‹¤.
        
        Args:
            pattern: íŒŒì¼ íŒ¨í„´ (glob í˜•ì‹)
            
        Returns:
            ë¡œë“œ ì„±ê³µ ì—¬ë¶€
        """
        try:
            files = glob.glob(pattern)
            
            if not files:
                print(f"âŒ íŒ¨í„´ì— ë§ëŠ” íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {pattern}")
                return False
            
            all_data = []
            
            for file_path in files:
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        file_data = json.load(f)
                    
                    if isinstance(file_data, list):
                        all_data.extend(file_data)
                    elif isinstance(file_data, dict):
                        all_data.append(file_data)
                    
                    print(f"ğŸ“„ {Path(file_path).name}: {len(file_data) if isinstance(file_data, list) else 1}ê°œ í•­ëª©")
                    
                except Exception as e:
                    print(f"âš ï¸  {Path(file_path).name} ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
            
            self.data = all_data
            print(f"âœ… JSON ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(self.data)}ê°œ í•­ëª©")
            return True
            
        except Exception as e:
            print(f"âŒ JSON íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
            return False
    
    def basic_statistics(self) -> Dict[str, Any]:
        """
        ê¸°ë³¸ í†µê³„ ì •ë³´ë¥¼ ìƒì„±í•œë‹¤.
        
        Returns:
            í†µê³„ ì •ë³´ ë”•ì…”ë„ˆë¦¬
        """
        if not self.data:
            return {"error": "ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤"}
        
        stats = {
            "total_products": len(self.data),
            "columns": set(),
            "branduid_stats": {},
            "item_name_stats": {},
            "price_stats": {},
            "category_stats": {},
            "option_stats": {},
            "images_stats": {},
            "celeb_stats": {},
            "origin_country_stats": {}
        }
        
        # ì»¬ëŸ¼ ìˆ˜ì§‘
        for item in self.data:
            stats["columns"].update(item.keys())
        stats["columns"] = list(stats["columns"])
        
        # branduid í†µê³„ (ì¤‘ë³µ ê²€ì‚¬ í¬í•¨)
        branduid_list = [item.get('branduid') for item in self.data if item.get('branduid')]
        unique_branduids = set(branduid_list)
        duplicates_count = len(branduid_list) - len(unique_branduids)
        
        # ì¤‘ë³µëœ branduid ì°¾ê¸°
        from collections import Counter
        branduid_counts = Counter(branduid_list)
        duplicate_branduids = {uid: count for uid, count in branduid_counts.items() if count > 1}
        
        stats["branduid_stats"] = {
            "count": len(branduid_list),
            "unique_count": len(unique_branduids),
            "duplicates_count": duplicates_count,
            "duplicate_branduids": duplicate_branduids,
            "sample": branduid_list[:5]
        }
        
        # ì œí’ˆëª… í†µê³„ (í˜„ì¬ ìŠ¤í‚¤ë§ˆ: item_name)
        item_names = [item.get('item_name') for item in self.data if item.get('item_name')]
        stats["item_name_stats"] = {
            "count": len(item_names),
            "empty_count": len(self.data) - len(item_names),
            "avg_length": sum(len(name) for name in item_names) / len(item_names) if item_names else 0,
            "sample": item_names[:3]
        }
        
        # ê°€ê²© í†µê³„
        prices = []
        for item in self.data:
            price = item.get('price')
            if isinstance(price, (int, float)) and price > 0:
                prices.append(price)
            elif isinstance(price, str):
                parsed_price = parse_price(price)
                if parsed_price:
                    prices.append(parsed_price)
        
        if prices:
            stats["price_stats"] = {
                "count": len(prices),
                "min": min(prices),
                "max": max(prices),
                "avg": sum(prices) / len(prices),
                "empty_count": len(self.data) - len(prices)
            }
        
        # ì¹´í…Œê³ ë¦¬ í†µê³„
        categories = [item.get('category_name') for item in self.data if item.get('category_name')]
        stats["category_stats"] = {
            "count": len(categories),
            "empty_count": len(self.data) - len(categories),
            "unique_categories": len(set(categories)),
            "distribution": self._get_most_common(categories, 10)
        }
        
        # ì˜µì…˜ í†µê³„ (ì•ˆì „í•œ ë¬¸ìì—´ ì²˜ë¦¬)
        products_with_options = [item for item in self.data if item.get('is_option_available')]
        option_info_present = [item for item in self.data if safe_str_check(item.get('option_info'))]
        
        stats["option_stats"] = {
            "products_marked_with_options": len(products_with_options),
            "products_with_option_info": len(option_info_present),
            "option_consistency_rate": (len(option_info_present) / len(products_with_options) * 100) if products_with_options else 0
        }
        
        # ì…€ëŸ½ ì •ë³´ í†µê³„ (ì•ˆì „í•œ ë¬¸ìì—´ ì²˜ë¦¬)
        celeb_info = []
        for item in self.data:
            celeb_value = item.get('related_celeb')
            if safe_str_check(celeb_value):
                if isinstance(celeb_value, str):
                    celeb_info.append(celeb_value.strip())
                else:
                    celeb_info.append(str(celeb_value).strip())
        
        stats["celeb_stats"] = {
            "count": len(celeb_info),
            "empty_count": len(self.data) - len(celeb_info),
            "percentage": (len(celeb_info) / len(self.data) * 100) if self.data else 0,
            "sample": celeb_info[:3]
        }
        
        # ì›ì‚°ì§€ í†µê³„ (ì•ˆì „í•œ ë¬¸ìì—´ ì²˜ë¦¬)
        origin_countries = []
        for item in self.data:
            origin_value = item.get('origin_country')
            if safe_str_check(origin_value):
                if isinstance(origin_value, str):
                    origin_countries.append(origin_value.strip())
                else:
                    origin_countries.append(str(origin_value).strip())
        
        stats["origin_country_stats"] = {
            "count": len(origin_countries),
            "empty_count": len(self.data) - len(origin_countries),
            "unique_countries": len(set(origin_countries)),
            "distribution": self._get_most_common(origin_countries, 10)
        }
        
        # ì´ë¯¸ì§€ í†µê³„ (í˜„ì¬ ìŠ¤í‚¤ë§ˆ: imagesëŠ” $$ë¡œ êµ¬ë¶„ëœ ë¬¸ìì—´)
        images_present = []
        for item in self.data:
            images = item.get('images', '')
            if images and images.strip():
                # $$ë¡œ êµ¬ë¶„ëœ ì´ë¯¸ì§€ URL ê°œìˆ˜ ê³„ì‚°
                image_count = len([img for img in images.split('$$') if img.strip()])
                images_present.append(image_count)
            else:
                images_present.append(0)
        
        if images_present:
            stats["images_stats"] = {
                "products_with_images": len([c for c in images_present if c > 0]),
                "total_images": sum(images_present),
                "avg_images_per_product": sum(images_present) / len(images_present),
                "max_images": max(images_present),
                "min_images": min(images_present)
            }
        
        return stats
    
    def _get_most_common(self, items: List[str], limit: int = 5) -> List[tuple]:
        """
        ê°€ì¥ ë§ì´ ë‚˜íƒ€ë‚˜ëŠ” í•­ëª©ë“¤ì„ ë°˜í™˜í•œë‹¤.
        
        Args:
            items: í•­ëª© ë¦¬ìŠ¤íŠ¸
            limit: ë°˜í™˜í•  ìµœëŒ€ ê°œìˆ˜
            
        Returns:
            (í•­ëª©, ê°œìˆ˜) íŠœí”Œ ë¦¬ìŠ¤íŠ¸
        """
        from collections import Counter
        counter = Counter(items)
        return counter.most_common(limit)
    
    def quality_analysis(self) -> Dict[str, Any]:
        """
        ë°ì´í„° í’ˆì§ˆ ë¶„ì„ì„ ìˆ˜í–‰í•œë‹¤.
        
        Returns:
            í’ˆì§ˆ ë¶„ì„ ê²°ê³¼
        """
        if not self.data:
            return {"error": "ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤"}
        
        quality = {
            "completeness": {},
            "validity": {},
            "consistency": {},
            "issues": []
        }
        
        # ì™„ì„±ë„ ë¶„ì„ (í˜„ì¬ ìŠ¤í‚¤ë§ˆì˜ í•„ìˆ˜ í•„ë“œë“¤) - ì•ˆì „í•œ ë¬¸ìì—´ ì²˜ë¦¬
        required_fields = ['branduid', 'item_name', 'price', 'category_name', 'images', 'origin_country']
        for field in required_fields:
            filled_count = len([item for item in self.data if safe_str_check(item.get(field))])
            quality["completeness"][field] = {
                "filled": filled_count,
                "empty": len(self.data) - filled_count,
                "percentage": (filled_count / len(self.data)) * 100
            }
        
        # ìœ íš¨ì„± ë¶„ì„
        # ê°€ê²© ìœ íš¨ì„±
        valid_prices = 0
        for item in self.data:
            price = item.get('price')
            if isinstance(price, (int, float)) and price > 0:
                valid_prices += 1
            elif isinstance(price, str) and parse_price(price):
                valid_prices += 1
        
        quality["validity"]["price"] = {
            "valid": valid_prices,
            "invalid": len(self.data) - valid_prices,
            "percentage": (valid_prices / len(self.data)) * 100
        }
        
        # ì¼ê´€ì„± ë¶„ì„
        # branduid ì¤‘ë³µ ê²€ì‚¬
        branduid_list = [item.get('branduid') for item in self.data if item.get('branduid')]
        duplicates = len(branduid_list) - len(set(branduid_list))
        quality["consistency"]["branduid_duplicates"] = duplicates
        
        # ì´ìŠˆ ë°œê²¬
        if duplicates > 0:
            quality["issues"].append(f"branduid ì¤‘ë³µ: {duplicates}ê°œ")
        
        empty_names = len(self.data) - len([item for item in self.data if item.get('item_name')])
        if empty_names > 0:
            quality["issues"].append(f"ì œí’ˆëª… ëˆ„ë½: {empty_names}ê°œ")
        
        # ì˜µì…˜ ì¼ì¹˜ì„± ê²€ì‚¬ (ì•ˆì „í•œ ë¬¸ìì—´ ì²˜ë¦¬)
        option_available_count = len([item for item in self.data if item.get('is_option_available')])
        option_info_count = len([item for item in self.data if safe_str_check(item.get('option_info'))])
        
        if option_available_count != option_info_count:
            quality["issues"].append(f"ì˜µì…˜ ì •ë³´ ë¶ˆì¼ì¹˜: ì˜µì…˜ ê°€ëŠ¥ {option_available_count}ê°œ vs ì˜µì…˜ ì •ë³´ {option_info_count}ê°œ")
        
        return quality
    
    def validation_analysis(self, validation_log_path: str = None) -> Dict[str, Any]:
        """
        ê²€ì¦ ë¡œê·¸ ë¶„ì„ì„ ìˆ˜í–‰í•œë‹¤.
        
        Args:
            validation_log_path: ê²€ì¦ ë¡œê·¸ íŒŒì¼ ê²½ë¡œ
            
        Returns:
            ê²€ì¦ ë¶„ì„ ê²°ê³¼
        """
        if not validation_log_path:
            # ì¸ìŠ¤í„´ìŠ¤ ë³€ìˆ˜ ë˜ëŠ” ê¸°ë³¸ ê²½ë¡œ ì‚¬ìš©
            validation_log_path = self.validation_log_path or "logs/final_validation_stats.json"
        
        try:
            with open(validation_log_path, 'r', encoding='utf-8') as f:
                validation_data = json.load(f)
            
            analysis = {
                "validation_summary": validation_data.get("summary", {}),
                "removal_breakdown": validation_data.get("removal_breakdown", {}),
                "validation_config": validation_data.get("validation_config", {}),
                "detailed_reasons": validation_data.get("detailed_reasons", [])
            }
            
            # ì œê±° ì´ìœ ë³„ í†µê³„ ë¶„ì„
            if analysis["detailed_reasons"]:
                reason_counts = {}
                for reason_item in analysis["detailed_reasons"]:
                    reason = reason_item.get("reason", "unknown")
                    reason_counts[reason] = reason_counts.get(reason, 0) + 1
                
                analysis["reason_distribution"] = reason_counts
            
            return analysis
            
        except FileNotFoundError:
            print(f"âš ï¸  ê²€ì¦ ë¡œê·¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {validation_log_path}")
            return {}
        except Exception as e:
            print(f"âŒ ê²€ì¦ ë¡œê·¸ ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
            return {}
    
    def remove_duplicates(self, save_deduplicated: bool = True, output_path: str = None) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:
        """
        branduid ì¤‘ë³µì„ ì œê±°í•˜ê³  ì •ë¦¬ëœ ë°ì´í„°ë¥¼ ë°˜í™˜í•œë‹¤.
        
        Args:
            save_deduplicated: ì¤‘ë³µ ì œê±°ëœ ë°ì´í„°ë¥¼ íŒŒì¼ë¡œ ì €ì¥í• ì§€ ì—¬ë¶€
            output_path: ì¤‘ë³µ ì œê±°ëœ ë°ì´í„° ì €ì¥ ê²½ë¡œ (ê¸°ë³¸: data/deduplicated_products.xlsx)
            
        Returns:
            (ì¤‘ë³µ_ì œê±°ëœ_ë°ì´í„°, ì¤‘ë³µ_ì œê±°_í†µê³„)
        """
        if not self.data:
            print("âŒ ì¤‘ë³µ ì œê±°í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ë°ì´í„°ë¥¼ ë¡œë“œí•˜ì„¸ìš”.")
            return [], {}
        
        print(f"ğŸ” branduid ì¤‘ë³µ ì œê±° ì‹œì‘: {len(self.data)}ê°œ ì œí’ˆ")
        
        # branduidë³„ë¡œ ì²« ë²ˆì§¸ í•­ëª©ë§Œ ìœ ì§€ (ìˆœì„œ ë³´ì¥)
        seen_branduids = set()
        deduplicated_data = []
        removed_items = []
        
        for item in self.data:
            branduid = item.get('branduid')
            if branduid and branduid not in seen_branduids:
                seen_branduids.add(branduid)
                deduplicated_data.append(item)
            elif branduid:
                removed_items.append(item)
                print(f"  ì¤‘ë³µ ì œê±°: {branduid}")
        
        # í†µê³„ ìƒì„±
        dedup_stats = {
            "original_count": len(self.data),
            "deduplicated_count": len(deduplicated_data),
            "removed_count": len(removed_items),
            "unique_branduids": len(seen_branduids)
        }
        
        print(f"âœ… ì¤‘ë³µ ì œê±° ì™„ë£Œ: {dedup_stats['original_count']} â†’ {dedup_stats['deduplicated_count']}ê°œ ({dedup_stats['removed_count']}ê°œ ì œê±°)")
        
        # ì¤‘ë³µ ì œê±°ëœ ë°ì´í„° ì €ì¥
        if save_deduplicated and deduplicated_data:
            if not output_path:
                output_path = "data/deduplicated_products.xlsx"
            
            try:
                from crawler.storage import ExcelStorage
                storage = ExcelStorage(output_path)
                storage.clear()  # ê¸°ì¡´ ë°ì´í„° ì‚­ì œ
                storage.save(deduplicated_data)
                print(f"ğŸ’¾ ì¤‘ë³µ ì œê±°ëœ ë°ì´í„° ì €ì¥: {output_path} ({len(deduplicated_data)}ê°œ ì œí’ˆ)")
            except Exception as e:
                print(f"âŒ ì¤‘ë³µ ì œê±°ëœ ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {str(e)}")
        
        return deduplicated_data, dedup_stats
    
    def validate_data(self, require_celeb_info: bool = True, save_validated: bool = True, output_path: str = None) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:
        """
        ë¡œë“œëœ ë°ì´í„°ì— ëŒ€í•´ ê²€ì¦ì„ ìˆ˜í–‰í•œë‹¤.
        
        Args:
            require_celeb_info: ì…€ëŸ½ ì •ë³´ í•„ìˆ˜ ì—¬ë¶€
            save_validated: ê²€ì¦ëœ ë°ì´í„°ë¥¼ íŒŒì¼ë¡œ ì €ì¥í• ì§€ ì—¬ë¶€
            output_path: ê²€ì¦ëœ ë°ì´í„° ì €ì¥ ê²½ë¡œ (ê¸°ë³¸: data/validated_products.xlsx)
            
        Returns:
            (ê²€ì¦ëœ_ë°ì´í„°, ê²€ì¦_í†µê³„)
        """
        if not self.data:
            print("âŒ ê²€ì¦í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ë°ì´í„°ë¥¼ ë¡œë“œí•˜ì„¸ìš”.")
            return [], {}
        
        print(f"ğŸ” ë°ì´í„° ê²€ì¦ ì‹œì‘: {len(self.data)}ê°œ ì œí’ˆ (ì…€ëŸ½ ì •ë³´ í•„ìˆ˜: {require_celeb_info})")
        
        # 1ë‹¨ê³„: ìë™ìœ¼ë¡œ ì¤‘ë³µ ì œê±° ìˆ˜í–‰
        print("1ï¸âƒ£ branduid ì¤‘ë³µ ì œê±° ì¤‘...")
        deduplicated_data, dedup_stats = self.remove_duplicates(save_deduplicated=False)
        
        if dedup_stats['removed_count'] > 0:
            print(f"   âœ… ì¤‘ë³µ ì œê±° ì™„ë£Œ: {dedup_stats['removed_count']}ê°œ ì œê±°")
            self.data = deduplicated_data  # ì¤‘ë³µ ì œê±°ëœ ë°ì´í„°ë¡œ êµì²´
        else:
            print("   âœ… ì¤‘ë³µ ì—†ìŒ")
        
        # 2ë‹¨ê³„: ê²€ì¦ê¸° ì´ˆê¸°í™” ë° ê²€ì¦ ìˆ˜í–‰
        print("2ï¸âƒ£ ë°ì´í„° í’ˆì§ˆ ê²€ì¦ ì¤‘...")
        self.validator = ProductValidator(require_celeb_info=require_celeb_info)
        
        # ê²€ì¦ ìˆ˜í–‰
        validated_products, validation_stats = self.validator.validate_products(self.data)
        
        # ê²€ì¦ ë³´ê³ ì„œ ìƒì„±
        validation_report = self.validator.generate_validation_report()
        print("\n" + validation_report)
        
        # ê²€ì¦ ë¡œê·¸ ì €ì¥
        from pathlib import Path
        logs_dir = Path("logs")
        logs_dir.mkdir(exist_ok=True)
        
        # ìƒì„¸ ë¡œê·¸ ì €ì¥
        log_file = logs_dir / "validation_stats.json"
        self.validator.save_validation_log(str(log_file))
        print(f"ğŸ“„ ê²€ì¦ ë¡œê·¸ ì €ì¥: {log_file}")
        
        # ë³´ê³ ì„œ ì €ì¥
        report_file = logs_dir / "validation_report.txt"
        try:
            with open(report_file, 'w', encoding='utf-8') as f:
                f.write(validation_report)
            print(f"ğŸ“„ ê²€ì¦ ë³´ê³ ì„œ ì €ì¥: {report_file}")
        except Exception as e:
            print(f"âš ï¸ ê²€ì¦ ë³´ê³ ì„œ ì €ì¥ ì‹¤íŒ¨: {str(e)}")
        
        # ê²€ì¦ëœ ë°ì´í„° ì €ì¥
        if save_validated and validated_products:
            if not output_path:
                output_path = "data/validated_products.xlsx"
            
            try:
                from crawler.storage import ExcelStorage
                storage = ExcelStorage(output_path)
                storage.clear()  # ê¸°ì¡´ ë°ì´í„° ì‚­ì œ
                storage.save(validated_products)
                print(f"âœ… ê²€ì¦ëœ ë°ì´í„° ì €ì¥: {output_path} ({len(validated_products)}ê°œ ì œí’ˆ)")
            except Exception as e:
                print(f"âŒ ê²€ì¦ëœ ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {str(e)}")
        
        return validated_products, validation_stats.to_dict()
    
    def generate_report(self, include_validation: bool = True) -> str:
        """
        ë¶„ì„ ë³´ê³ ì„œë¥¼ ìƒì„±í•œë‹¤.
        
        Args:
            include_validation: ê²€ì¦ ë¶„ì„ í¬í•¨ ì—¬ë¶€
        
        Returns:
            ë¶„ì„ ë³´ê³ ì„œ í…ìŠ¤íŠ¸
        """
        stats = self.basic_statistics()
        quality = self.quality_analysis()
        validation = self.validation_analysis() if include_validation else {}
        
        report = []
        report.append("ğŸ“Š í¬ë¡¤ë§ ë°ì´í„° ë¶„ì„ ë³´ê³ ì„œ")
        report.append("=" * 50)
        report.append("")
        
        # ê¸°ë³¸ í†µê³„
        report.append("ğŸ“ˆ ê¸°ë³¸ í†µê³„:")
        report.append(f"  ì´ ì œí’ˆ ìˆ˜: {stats.get('total_products', 0):,}ê°œ")
        report.append(f"  ë°ì´í„° ì»¬ëŸ¼: {', '.join(stats.get('columns', []))}")
        report.append("")
        
        # branduid í†µê³„ (ì¤‘ë³µ ì •ë³´ í¬í•¨)
        if 'branduid_stats' in stats:
            bs = stats['branduid_stats']
            report.append("ğŸ†” Branduid í†µê³„:")
            report.append(f"  ì´ ê°œìˆ˜: {bs.get('count', 0)}ê°œ")
            report.append(f"  ê³ ìœ  ê°œìˆ˜: {bs.get('unique_count', 0)}ê°œ")
            
            duplicates_count = bs.get('duplicates_count', 0)
            if duplicates_count > 0:
                report.append(f"  âš ï¸ ì¤‘ë³µ ê°œìˆ˜: {duplicates_count}ê°œ")
                duplicate_branduids = bs.get('duplicate_branduids', {})
                if duplicate_branduids:
                    report.append("  ì¤‘ë³µëœ branduid (ìƒìœ„ 5ê°œ):")
                    for uid, count in list(duplicate_branduids.items())[:5]:
                        report.append(f"    - {uid}: {count}íšŒ ì¤‘ë³µ")
            else:
                report.append("  âœ… ì¤‘ë³µ ì—†ìŒ")
            report.append("")
        
        # ì œí’ˆëª… í†µê³„
        if 'item_name_stats' in stats:
            ns = stats['item_name_stats']
            report.append("ğŸ“ ì œí’ˆëª… í†µê³„:")
            report.append(f"  ì œí’ˆëª… ìˆìŒ: {ns.get('count', 0)}ê°œ")
            report.append(f"  ì œí’ˆëª… ì—†ìŒ: {ns.get('empty_count', 0)}ê°œ")
            report.append(f"  í‰ê·  ê¸¸ì´: {ns.get('avg_length', 0):.1f}ì")
            report.append("")
        
        # ê°€ê²© í†µê³„
        if 'price_stats' in stats:
            ps = stats['price_stats']
            report.append("ğŸ’° ê°€ê²© í†µê³„:")
            report.append(f"  ê°€ê²© ìˆìŒ: {ps.get('count', 0)}ê°œ")
            report.append(f"  ìµœì €ê°€: {ps.get('min', 0):,}ì›")
            report.append(f"  ìµœê³ ê°€: {ps.get('max', 0):,}ì›")
            report.append(f"  í‰ê· ê°€: {ps.get('avg', 0):,.0f}ì›")
            report.append("")
        
        # ì¹´í…Œê³ ë¦¬ í†µê³„
        if 'category_stats' in stats:
            cs = stats['category_stats']
            report.append("ğŸ“‚ ì¹´í…Œê³ ë¦¬ í†µê³„:")
            report.append(f"  ì¹´í…Œê³ ë¦¬ ìˆìŒ: {cs.get('count', 0)}ê°œ")
            report.append(f"  ì¹´í…Œê³ ë¦¬ ì—†ìŒ: {cs.get('empty_count', 0)}ê°œ")
            report.append(f"  ê³ ìœ  ì¹´í…Œê³ ë¦¬: {cs.get('unique_categories', 0)}ê°œ")
            if cs.get('distribution'):
                report.append("  ì¹´í…Œê³ ë¦¬ ë¶„í¬:")
                for category, count in cs['distribution']:
                    report.append(f"    - {category}: {count}ê°œ")
            report.append("")

        # ì˜µì…˜ í†µê³„
        if 'option_stats' in stats:
            os = stats['option_stats']
            report.append("ğŸ¨ ì˜µì…˜ í†µê³„:")
            report.append(f"  ì˜µì…˜ ê°€ëŠ¥ìœ¼ë¡œ í‘œì‹œëœ ì œí’ˆ: {os.get('products_marked_with_options', 0)}ê°œ")
            report.append(f"  ì˜µì…˜ ì •ë³´ê°€ ìˆëŠ” ì œí’ˆ: {os.get('products_with_option_info', 0)}ê°œ")
            report.append(f"  ì˜µì…˜ ì¼ì¹˜ìœ¨: {os.get('option_consistency_rate', 0):.1f}%")
            report.append("")
        
        # ì´ë¯¸ì§€ í†µê³„
        if 'images_stats' in stats:
            imgs = stats['images_stats']
            report.append("ğŸ–¼ï¸  ì´ë¯¸ì§€ í†µê³„:")
            report.append(f"  ì´ë¯¸ì§€ ìˆëŠ” ì œí’ˆ: {imgs.get('products_with_images', 0)}ê°œ")
            report.append(f"  ì´ ì´ë¯¸ì§€ ìˆ˜: {imgs.get('total_images', 0)}ê°œ")
            report.append(f"  ì œí’ˆë‹¹ í‰ê·  ì´ë¯¸ì§€: {imgs.get('avg_images_per_product', 0):.1f}ê°œ")
            report.append("")
        
        # ì…€ëŸ½ ì •ë³´ í†µê³„
        if 'celeb_stats' in stats:
            celeb = stats['celeb_stats']
            report.append("â­ ì…€ëŸ½ ì •ë³´ í†µê³„:")
            report.append(f"  ì…€ëŸ½ ì •ë³´ ìˆìŒ: {celeb.get('count', 0)}ê°œ")
            report.append(f"  ì…€ëŸ½ ì •ë³´ ì—†ìŒ: {celeb.get('empty_count', 0)}ê°œ")
            report.append(f"  ì…€ëŸ½ ì •ë³´ ë¹„ìœ¨: {celeb.get('percentage', 0):.1f}%")
            report.append("")
        
        # ì›ì‚°ì§€ í†µê³„
        if 'origin_country_stats' in stats:
            origin = stats['origin_country_stats']
            report.append("ğŸŒ ì›ì‚°ì§€ í†µê³„:")
            report.append(f"  ì›ì‚°ì§€ ì •ë³´ ìˆìŒ: {origin.get('count', 0)}ê°œ")
            report.append(f"  ì›ì‚°ì§€ ì •ë³´ ì—†ìŒ: {origin.get('empty_count', 0)}ê°œ")
            report.append(f"  ê³ ìœ  ì›ì‚°ì§€: {origin.get('unique_countries', 0)}ê°œ")
            if origin.get('distribution'):
                report.append("  ì›ì‚°ì§€ ë¶„í¬:")
                for country, count in origin['distribution']:
                    report.append(f"    - {country}: {count}ê°œ")
            report.append("")
        
        # í’ˆì§ˆ ë¶„ì„
        report.append("ğŸ” ë°ì´í„° í’ˆì§ˆ ë¶„ì„:")
        if 'completeness' in quality:
            for field, comp in quality['completeness'].items():
                report.append(f"  {field} ì™„ì„±ë„: {comp.get('percentage', 0):.1f}% ({comp.get('filled', 0)}/{comp.get('filled', 0) + comp.get('empty', 0)})")
        
        if quality.get('issues'):
            report.append("")
            report.append("âš ï¸  ë°œê²¬ëœ ì´ìŠˆ:")
            for issue in quality['issues']:
                report.append(f"  - {issue}")
        
        # ê²€ì¦ ë¶„ì„ ê²°ê³¼ ì¶”ê°€
        if validation and include_validation:
            report.append("")
            report.append("ğŸ” ë°ì´í„° ê²€ì¦ ê²°ê³¼:")
            
            validation_summary = validation.get("validation_summary", {})
            if validation_summary:
                report.append(f"  ê²€ì¦ ì²˜ë¦¬ëœ ì œí’ˆ ìˆ˜: {validation_summary.get('total_products', 0):,}ê°œ")
                report.append(f"  ê²€ì¦ í†µê³¼ ì œí’ˆ ìˆ˜: {validation_summary.get('valid_products', 0):,}ê°œ")
                report.append(f"  ì œê±°ëœ ì œí’ˆ ìˆ˜: {validation_summary.get('removed_products', 0):,}ê°œ")
                report.append(f"  ê²€ì¦ ì„±ê³µë¥ : {validation_summary.get('success_rate', 0):.1f}%")
            
            removal_breakdown = validation.get("removal_breakdown", {})
            if removal_breakdown:
                report.append("")
                report.append("ğŸ“‹ ì œê±° ì´ìœ ë³„ í†µê³„:")
                for reason, count in removal_breakdown.items():
                    if count > 0:
                        reason_names = {
                            "missing_required_fields": "í•„ìˆ˜ í•„ë“œ ëˆ„ë½",
                            "invalid_price": "ìœ íš¨í•˜ì§€ ì•Šì€ ê°€ê²©",
                            "missing_images": "ì´ë¯¸ì§€ ëˆ„ë½",
                            "missing_origin_country": "ì›ì‚°ì§€ ì •ë³´ ëˆ„ë½",
                            "option_inconsistency": "ì˜µì…˜ ì •ë³´ ë¶ˆì¼ì¹˜",
                            "discontinued_products": "íŒë§¤ì¢…ë£Œ ìƒí’ˆ",
                            "missing_celeb_info": "ì…€ëŸ½ ì •ë³´ ëˆ„ë½"
                        }
                        reason_name = reason_names.get(reason, reason)
                        report.append(f"  {reason_name}: {count}ê°œ")
            
            validation_config = validation.get("validation_config", {})
            if validation_config:
                report.append("")
                report.append("âš™ï¸ ê²€ì¦ ì„¤ì •:")
                required_fields = validation_config.get("required_fields", [])
                if required_fields:
                    report.append(f"  í•„ìˆ˜ í•„ë“œ: {', '.join(required_fields)}")
                require_celeb = validation_config.get("require_celeb_info", True)
                report.append(f"  ì…€ëŸ½ ì •ë³´ í•„ìˆ˜: {'ì˜ˆ' if require_celeb else 'ì•„ë‹ˆì˜¤'}")
        
        return "\n".join(report)


def save_report(report: str, output_file: str = "playground/results/analysis_report.txt"):
    """
    ë¶„ì„ ë³´ê³ ì„œë¥¼ íŒŒì¼ë¡œ ì €ì¥í•œë‹¤.
    
    Args:
        report: ë³´ê³ ì„œ í…ìŠ¤íŠ¸
        output_file: ì €ì¥í•  íŒŒì¼ ê²½ë¡œ
    """
    output_path = Path(output_file)
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(report)
    
    print(f"ğŸ’¾ ë³´ê³ ì„œ ì €ì¥ ì™„ë£Œ: {output_path}")


def main():
    """ë©”ì¸ í•¨ìˆ˜."""
    parser = argparse.ArgumentParser(description="í¬ë¡¤ë§ ë°ì´í„° ë¶„ì„ ë„êµ¬")
    parser.add_argument(
        "--input",
        required=True,
        help="ë¶„ì„í•  ë°ì´í„° íŒŒì¼ (Excel ë˜ëŠ” JSON íŒ¨í„´)"
    )
    parser.add_argument(
        "--format",
        choices=["excel", "json"],
        help="ì…ë ¥ íŒŒì¼ í˜•ì‹ (ìë™ ê°ì§€ë¨)"
    )
    parser.add_argument(
        "--output",
        default="playground/results/analysis_report.txt",
        help="ë³´ê³ ì„œ ì €ì¥ íŒŒì¼"
    )
    parser.add_argument(
        "--detailed",
        action="store_true",
        help="ìƒì„¸ ë¶„ì„ ëª¨ë“œ"
    )
    parser.add_argument(
        "--validation-log",
        help="ê²€ì¦ ë¡œê·¸ íŒŒì¼ ê²½ë¡œ (ê¸°ë³¸: logs/final_validation_stats.json)"
    )
    parser.add_argument(
        "--no-validation",
        action="store_true",
        help="ê²€ì¦ ë¶„ì„ ì œì™¸"
    )
    parser.add_argument(
        "--validate",
        action="store_true",
        help="ë°ì´í„° ê²€ì¦ ìˆ˜í–‰"
    )
    parser.add_argument(
        "--require-celeb-info",
        action="store_true",
        help="ì…€ëŸ½ ì •ë³´ë¥¼ í•„ìˆ˜ë¡œ ì„¤ì •"
    )
    parser.add_argument(
        "--validated-output",
        help="ê²€ì¦ëœ ë°ì´í„° ì €ì¥ ê²½ë¡œ (ê¸°ë³¸: data/validated_products.xlsx)"
    )
    parser.add_argument(
        "--remove-duplicates",
        action="store_true",
        help="branduid ì¤‘ë³µ ì œê±° ìˆ˜í–‰"
    )
    parser.add_argument(
        "--deduplicated-output",
        help="ì¤‘ë³µ ì œê±°ëœ ë°ì´í„° ì €ì¥ ê²½ë¡œ (ê¸°ë³¸: data/deduplicated_products.xlsx)"
    )
    
    args = parser.parse_args()
    
    print("ğŸ“Š ë°ì´í„° ë¶„ì„ í”Œë ˆì´ê·¸ë¼ìš´ë“œ")
    print("=" * 40)
    
    analyzer = DataAnalyzer()
    
    # íŒŒì¼ í˜•ì‹ ìë™ ê°ì§€
    input_path = args.input
    if args.format == "json" or "*" in input_path or input_path.endswith(".json"):
        success = analyzer.load_json_files(input_path)
    elif args.format == "excel" or input_path.endswith((".xlsx", ".xls")):
        success = analyzer.load_excel(input_path)
    else:
        print("âŒ íŒŒì¼ í˜•ì‹ì„ ê°ì§€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. --format ì˜µì…˜ì„ ì‚¬ìš©í•˜ì„¸ìš”.")
        sys.exit(1)
    
    if not success:
        sys.exit(1)
    
    # ê²€ì¦ ë¡œê·¸ ê²½ë¡œ ì„¤ì •
    if args.validation_log:
        analyzer.validation_log_path = args.validation_log
    
    # ì¤‘ë³µ ì œê±° ìˆ˜í–‰ (ìš”ì²­ëœ ê²½ìš°)
    if args.remove_duplicates:
        print("\nğŸ”„ branduid ì¤‘ë³µ ì œê±° ìˆ˜í–‰ ì¤‘...")
        deduplicated_data, dedup_stats = analyzer.remove_duplicates(
            save_deduplicated=True,
            output_path=args.deduplicated_output
        )
        
        # ì¤‘ë³µ ì œê±°ëœ ë°ì´í„°ë¡œ êµì²´
        analyzer.data = deduplicated_data
        print(f"ğŸ“Š ë¶„ì„ ëŒ€ìƒ ë°ì´í„° ì—…ë°ì´íŠ¸: {dedup_stats['deduplicated_count']}ê°œ ì œí’ˆ")
    
    # ë°ì´í„° ê²€ì¦ ìˆ˜í–‰ (ìš”ì²­ëœ ê²½ìš°) - ìë™ìœ¼ë¡œ ì¤‘ë³µ ì œê±° í¬í•¨
    if args.validate:
        print("\nğŸ” ë°ì´í„° ê²€ì¦ ìˆ˜í–‰ ì¤‘...")
        _, validation_stats = analyzer.validate_data(
            require_celeb_info=args.require_celeb_info,
            save_validated=True,
            output_path=args.validated_output
        )
        
        # ê²€ì¦ í›„ validated ë°ì´í„° ê²½ë¡œ ì„¤ì • (ë³´ê³ ì„œì—ì„œ ê²€ì¦ ê²°ê³¼ë¥¼ í‘œì‹œí•˜ê¸° ìœ„í•´)
        analyzer.validation_log_path = "logs/validation_stats.json"
    
    # ë¶„ì„ ìˆ˜í–‰
    print("\nğŸ” ë¶„ì„ ìˆ˜í–‰ ì¤‘...")
    include_validation = not args.no_validation
    report = analyzer.generate_report(include_validation=include_validation)
    
    # ê²°ê³¼ ì¶œë ¥
    print("\n" + report)
    
    # íŒŒì¼ ì €ì¥
    save_report(report, args.output)
    
    if args.detailed:
        print("\nğŸ“‹ ìƒì„¸ í†µê³„ (JSON):")
        stats = analyzer.basic_statistics()
        print(json.dumps(stats, ensure_ascii=False, indent=2))


if __name__ == "__main__":
    main()